{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import spacy\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from time import time \n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "import keras \n",
    "from keras.models import Sequential, Model \n",
    "from keras import layers\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, Input, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install -c conda-forge wordcloud --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/vn0w1vm/Downloads/train_pa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1\n",
       "1  Ever get the feeling your life circles the rou...      0\n",
       "2  Why the Truth Might Get You Fired October 29, ...      1\n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1\n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rename(columns ={\"text\":\"review\"}, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to tokenize everything: 0.07 mins\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>clean</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>[House, Dem, Aide, We, Didn, t, Even, See, Com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>[Ever, get, the, feeling, your, life, circles,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>[Why, the, Truth, Might, Get, You, Fired, Octo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>[Videos, 15, Civilians, Killed, In, Single, US...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>[Print, An, Iranian, woman, has, been, sentenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label  \\\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1   \n",
       "1  Ever get the feeling your life circles the rou...      0   \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1   \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1   \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1   \n",
       "\n",
       "                                               clean  \\\n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1  Ever get the feeling your life circles the rou...   \n",
       "2  Why the Truth Might Get You Fired October 29, ...   \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...   \n",
       "4  Print \\nAn Iranian woman has been sentenced to...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [House, Dem, Aide, We, Didn, t, Even, See, Com...  \n",
       "1  [Ever, get, the, feeling, your, life, circles,...  \n",
       "2  [Why, the, Truth, Might, Get, You, Fired, Octo...  \n",
       "3  [Videos, 15, Civilians, Killed, In, Single, US...  \n",
       "4  [Print, An, Iranian, woman, has, been, sentenc...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = data.copy()\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "t = time()\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df_clean['clean'] = df_clean['review'].astype('str') \n",
    "df_clean.dtypes\n",
    "\n",
    "df_clean[\"tokens\"] = df_clean[\"clean\"].apply(tokenizer.tokenize)\n",
    "# delete Stop Words\n",
    "\n",
    "print('Time to tokenize everything: {} mins'.format(round((time() - t) / 60, 2)))\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First defining the X (input), and the y (output)\n",
    "y = data['label'].values\n",
    "X = np.array(df_clean[\"tokens\"])\n",
    "\n",
    "#And here is the train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size : 33731\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x for x in X_train])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print ('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += w2v_model[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape for training set :  (16640, 300) \n",
      "shape for test set :  (4160, 300)\n"
     ]
    }
   ],
   "source": [
    "train_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in map(lambda x: x, X_train)])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, 300) for z in map(lambda x: x, X_test)])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)\n",
    "\n",
    "print ('shape for training set : ',train_vecs_w2v.shape,\n",
    "      '\\nshape for test set : ', test_vecs_w2v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 128)               38528     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 38,657\n",
      "Trainable params: 38,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, activation='relu', input_dim=300))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16640 samples, validate on 4160 samples\n",
      "Epoch 1/20\n",
      "16640/16640 [==============================] - 1s 37us/step - loss: 0.4536 - accuracy: 0.7974 - val_loss: 0.3257 - val_accuracy: 0.8632\n",
      "Epoch 2/20\n",
      "16640/16640 [==============================] - 1s 48us/step - loss: 0.3221 - accuracy: 0.8646 - val_loss: 0.2958 - val_accuracy: 0.8800\n",
      "Epoch 3/20\n",
      "16640/16640 [==============================] - 1s 49us/step - loss: 0.2900 - accuracy: 0.8810 - val_loss: 0.2833 - val_accuracy: 0.8875\n",
      "Epoch 4/20\n",
      "16640/16640 [==============================] - 1s 34us/step - loss: 0.2698 - accuracy: 0.8907 - val_loss: 0.2701 - val_accuracy: 0.8959\n",
      "Epoch 5/20\n",
      "16640/16640 [==============================] - 1s 40us/step - loss: 0.2558 - accuracy: 0.8951 - val_loss: 0.2672 - val_accuracy: 0.8990\n",
      "Epoch 6/20\n",
      "16640/16640 [==============================] - 1s 35us/step - loss: 0.2513 - accuracy: 0.9037 - val_loss: 0.2679 - val_accuracy: 0.8988\n",
      "Epoch 7/20\n",
      "16640/16640 [==============================] - 1s 34us/step - loss: 0.2351 - accuracy: 0.9058 - val_loss: 0.2595 - val_accuracy: 0.9036\n",
      "Epoch 8/20\n",
      "16640/16640 [==============================] - 1s 31us/step - loss: 0.2303 - accuracy: 0.9087 - val_loss: 0.2553 - val_accuracy: 0.9062\n",
      "Epoch 9/20\n",
      "16640/16640 [==============================] - 1s 43us/step - loss: 0.2234 - accuracy: 0.9124 - val_loss: 0.2535 - val_accuracy: 0.9089\n",
      "Epoch 10/20\n",
      "16640/16640 [==============================] - 1s 32us/step - loss: 0.2196 - accuracy: 0.9143 - val_loss: 0.2546 - val_accuracy: 0.9087\n",
      "Epoch 11/20\n",
      "16640/16640 [==============================] - 0s 26us/step - loss: 0.2119 - accuracy: 0.9160 - val_loss: 0.2579 - val_accuracy: 0.9062\n",
      "Epoch 12/20\n",
      "16640/16640 [==============================] - 1s 34us/step - loss: 0.2105 - accuracy: 0.9150 - val_loss: 0.2530 - val_accuracy: 0.9111\n",
      "Epoch 13/20\n",
      "16640/16640 [==============================] - 0s 28us/step - loss: 0.2023 - accuracy: 0.9206 - val_loss: 0.2583 - val_accuracy: 0.9103\n",
      "Epoch 14/20\n",
      "16640/16640 [==============================] - 1s 37us/step - loss: 0.1971 - accuracy: 0.9243 - val_loss: 0.2533 - val_accuracy: 0.9106\n",
      "Epoch 15/20\n",
      "16640/16640 [==============================] - 0s 29us/step - loss: 0.1943 - accuracy: 0.9254 - val_loss: 0.2520 - val_accuracy: 0.9144\n",
      "Epoch 16/20\n",
      "16640/16640 [==============================] - 1s 31us/step - loss: 0.1962 - accuracy: 0.9242 - val_loss: 0.2539 - val_accuracy: 0.9147\n",
      "Epoch 17/20\n",
      "16640/16640 [==============================] - 0s 24us/step - loss: 0.1871 - accuracy: 0.9293 - val_loss: 0.2501 - val_accuracy: 0.9161\n",
      "Epoch 18/20\n",
      "16640/16640 [==============================] - 0s 29us/step - loss: 0.1873 - accuracy: 0.9290 - val_loss: 0.2568 - val_accuracy: 0.9137\n",
      "Epoch 19/20\n",
      "16640/16640 [==============================] - 1s 47us/step - loss: 0.1823 - accuracy: 0.9302 - val_loss: 0.2638 - val_accuracy: 0.9168\n",
      "Epoch 20/20\n",
      "16640/16640 [==============================] - 1s 35us/step - loss: 0.1781 - accuracy: 0.9295 - val_loss: 0.2614 - val_accuracy: 0.9156\n",
      "Training Accuracy: 0.9586\n",
      "Testing Accuracy:  0.9156\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_vecs_w2v, y_train, epochs=20, batch_size=50,\n",
    "                   validation_data=(test_vecs_w2v,y_test))\n",
    "loss, accuracy = model.evaluate(train_vecs_w2v, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(test_vecs_w2v, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# def plot_history(history):\n",
    "#     acc = history.history['accuracy']\n",
    "#     val_acc = history.history['val_accuracy']\n",
    "#     loss = history.history['loss']\n",
    "#     val_loss = history.history['val_loss']\n",
    "#     x = range(1, len(acc) + 1)\n",
    "\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(x, acc, 'b', label='Training acc')\n",
    "#     plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "#     plt.title('Training and validation accuracy')\n",
    "#     plt.legend()\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(x, loss, 'b', label='Training loss')\n",
    "#     plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "#     plt.title('Training and validation loss')\n",
    "#     plt.legend()\n",
    "# plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15981944 words total, with a vocabulary size of 216124\n",
      "Max sentence length is 24814\n"
     ]
    }
   ],
   "source": [
    "all_words = [word for tokens in X for word in tokens]\n",
    "all_sentence_lengths = [len(tokens) for tokens in X]\n",
    "ALL_VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(ALL_VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(all_sentence_lengths))\n",
    "\n",
    "\n",
    "####################### CHANGE THE PARAMETERS HERE #####################################\n",
    "EMBEDDING_DIM = 300 # how big is each word vector\n",
    "MAX_VOCAB_SIZE = 18399# how many unique words to use (i.e num rows in embedding vector)\n",
    "MAX_SEQUENCE_LENGTH = 53 # max number of words in a comment to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 238051 unique tokens.\n",
      "(238052, 300)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, lower=True, char_level=False)\n",
    "data[\"review\"] = data[\"review\"].astype(str)\n",
    "tokenizer.fit_on_texts(data[\"review\"].tolist())\n",
    "training_sequences = tokenizer.texts_to_sequences(X_train.tolist())\n",
    "\n",
    "train_word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(train_word_index))\n",
    "\n",
    "train_embedding_weights = np.zeros((len(train_word_index)+1, EMBEDDING_DIM))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embedding_weights[index,:] = w2v_model[word] if word in w2v_model else np.random.rand(EMBEDDING_DIM)\n",
    "print(train_embedding_weights.shape)\n",
    "\n",
    "\n",
    "######################## TRAIN AND TEST SET #################################\n",
    "train_cnn_data = pad_sequences(training_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test.tolist())\n",
    "test_cnn_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16640, 53), (4160, 53))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cnn_data.shape, test_cnn_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import concatenate\n",
    "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, trainable=False, extra_conv=True):\n",
    "    \n",
    "    embedding_layer = Embedding(num_words,\n",
    "                            embedding_dim,\n",
    "                            weights=[embeddings],\n",
    "                            input_length=max_sequence_length,\n",
    "                            trainable=trainable)\n",
    "\n",
    "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "    embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
    "    convs = []\n",
    "    filter_sizes = [3,4,5]\n",
    "\n",
    "    for filter_size in filter_sizes:\n",
    "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
    "        l_pool = MaxPooling1D(pool_size=3)(l_conv)\n",
    "        convs.append(l_pool)\n",
    "\n",
    "    l_merge = concatenate([convs[0],convs[1],convs[2]],axis=1)\n",
    "\n",
    "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
    "    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
    "    pool = MaxPooling1D(pool_size=3)(conv)\n",
    "\n",
    "    if extra_conv==True:\n",
    "        x = Dropout(0.5)(l_merge)  \n",
    "    else:\n",
    "        # Original Yoon Kim model\n",
    "        x = Dropout(0.5)(pool)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    # Finally, we feed the output into a Sigmoid layer.\n",
    "    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n",
    "    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n",
    "    preds = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(sequence_input, preds)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 53)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 53, 300)      71415600    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 51, 128)      115328      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 50, 128)      153728      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 49, 128)      192128      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 17, 128)      0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 16, 128)      0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 16, 128)      0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 49, 128)      0           max_pooling1d_5[0][0]            \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "                                                                 max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 49, 128)      0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 6272)         0           dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 128)          802944      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            129         dense_11[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 72,679,857\n",
      "Trainable params: 1,264,257\n",
      "Non-trainable params: 71,415,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16640 samples, validate on 4160 samples\n",
      "Epoch 1/100\n",
      "16640/16640 [==============================] - 11s 664us/step - loss: 0.4799 - acc: 0.7544 - val_loss: 0.3390 - val_acc: 0.8375\n",
      "Epoch 2/100\n",
      "16640/16640 [==============================] - 11s 647us/step - loss: 0.2908 - acc: 0.8730 - val_loss: 0.2968 - val_acc: 0.8608\n",
      "Epoch 3/100\n",
      "16640/16640 [==============================] - 10s 599us/step - loss: 0.2116 - acc: 0.9105 - val_loss: 0.2882 - val_acc: 0.8649\n",
      "Epoch 4/100\n",
      "16640/16640 [==============================] - 11s 640us/step - loss: 0.1397 - acc: 0.9448 - val_loss: 0.3027 - val_acc: 0.8673\n",
      "Epoch 5/100\n",
      "16640/16640 [==============================] - 11s 669us/step - loss: 0.0906 - acc: 0.9661 - val_loss: 0.3535 - val_acc: 0.8651\n",
      "Epoch 6/100\n",
      "16640/16640 [==============================] - 10s 611us/step - loss: 0.0684 - acc: 0.9744 - val_loss: 0.4086 - val_acc: 0.8680\n",
      "Epoch 7/100\n",
      "16640/16640 [==============================] - 11s 640us/step - loss: 0.0630 - acc: 0.9755 - val_loss: 0.4097 - val_acc: 0.8702\n",
      "Epoch 8/100\n",
      "16640/16640 [==============================] - 11s 657us/step - loss: 0.0469 - acc: 0.9829 - val_loss: 0.4470 - val_acc: 0.8661\n",
      "Epoch 9/100\n",
      "16640/16640 [==============================] - 10s 628us/step - loss: 0.0421 - acc: 0.9851 - val_loss: 0.4889 - val_acc: 0.8649\n",
      "Epoch 10/100\n",
      "16640/16640 [==============================] - 10s 627us/step - loss: 0.0419 - acc: 0.9848 - val_loss: 0.4565 - val_acc: 0.8714\n",
      "Epoch 11/100\n",
      "16640/16640 [==============================] - 10s 623us/step - loss: 0.0428 - acc: 0.9839 - val_loss: 0.4759 - val_acc: 0.8736\n",
      "Epoch 12/100\n",
      "16640/16640 [==============================] - 11s 683us/step - loss: 0.0326 - acc: 0.9885 - val_loss: 0.5028 - val_acc: 0.8678\n",
      "Epoch 13/100\n",
      "16640/16640 [==============================] - 11s 656us/step - loss: 0.0330 - acc: 0.9885 - val_loss: 0.5423 - val_acc: 0.8707\n",
      "Epoch 14/100\n",
      "16640/16640 [==============================] - 11s 642us/step - loss: 0.0336 - acc: 0.9881 - val_loss: 0.5112 - val_acc: 0.8716\n",
      "Epoch 15/100\n",
      "16640/16640 [==============================] - 11s 654us/step - loss: 0.0325 - acc: 0.9886 - val_loss: 0.4647 - val_acc: 0.8731\n",
      "Epoch 16/100\n",
      "16640/16640 [==============================] - 12s 695us/step - loss: 0.0281 - acc: 0.9900 - val_loss: 0.5051 - val_acc: 0.8748\n",
      "Epoch 17/100\n",
      "16640/16640 [==============================] - 11s 648us/step - loss: 0.0240 - acc: 0.9919 - val_loss: 0.5265 - val_acc: 0.8760\n",
      "Epoch 18/100\n",
      "16640/16640 [==============================] - 11s 659us/step - loss: 0.0251 - acc: 0.9914 - val_loss: 0.5239 - val_acc: 0.8690\n",
      "Epoch 19/100\n",
      "16640/16640 [==============================] - 11s 648us/step - loss: 0.0233 - acc: 0.9912 - val_loss: 0.5265 - val_acc: 0.8707\n",
      "Epoch 20/100\n",
      "16640/16640 [==============================] - 11s 684us/step - loss: 0.0253 - acc: 0.9907 - val_loss: 0.5289 - val_acc: 0.8700\n",
      "Epoch 21/100\n",
      "16640/16640 [==============================] - 11s 682us/step - loss: 0.0246 - acc: 0.9911 - val_loss: 0.5447 - val_acc: 0.8714\n",
      "Epoch 22/100\n",
      "16640/16640 [==============================] - 11s 655us/step - loss: 0.0219 - acc: 0.9921 - val_loss: 0.5585 - val_acc: 0.8738\n",
      "Epoch 23/100\n",
      "16640/16640 [==============================] - 12s 702us/step - loss: 0.0216 - acc: 0.9919 - val_loss: 0.5496 - val_acc: 0.8697\n",
      "Epoch 24/100\n",
      "16640/16640 [==============================] - 10s 627us/step - loss: 0.0183 - acc: 0.9941 - val_loss: 0.5376 - val_acc: 0.8738\n",
      "Epoch 25/100\n",
      "16640/16640 [==============================] - 10s 611us/step - loss: 0.0204 - acc: 0.9925 - val_loss: 0.5600 - val_acc: 0.8712\n",
      "Epoch 26/100\n",
      "16640/16640 [==============================] - 11s 661us/step - loss: 0.0195 - acc: 0.9925 - val_loss: 0.4965 - val_acc: 0.8755\n",
      "Epoch 27/100\n",
      "16640/16640 [==============================] - 11s 639us/step - loss: 0.0167 - acc: 0.9942 - val_loss: 0.5865 - val_acc: 0.8707\n",
      "Epoch 28/100\n",
      "16640/16640 [==============================] - 11s 636us/step - loss: 0.0180 - acc: 0.9934 - val_loss: 0.5749 - val_acc: 0.8796\n",
      "Epoch 29/100\n",
      "16640/16640 [==============================] - 12s 710us/step - loss: 0.0156 - acc: 0.9945 - val_loss: 0.5548 - val_acc: 0.8762\n",
      "Epoch 30/100\n",
      "16640/16640 [==============================] - 12s 696us/step - loss: 0.0198 - acc: 0.9931 - val_loss: 0.6155 - val_acc: 0.8702\n",
      "Epoch 31/100\n",
      "16640/16640 [==============================] - 11s 655us/step - loss: 0.0201 - acc: 0.9928 - val_loss: 0.5512 - val_acc: 0.8731\n",
      "Epoch 32/100\n",
      "16640/16640 [==============================] - 12s 725us/step - loss: 0.0126 - acc: 0.9960 - val_loss: 0.6194 - val_acc: 0.8668\n",
      "Epoch 33/100\n",
      "16640/16640 [==============================] - 10s 591us/step - loss: 0.0147 - acc: 0.9948 - val_loss: 0.6211 - val_acc: 0.8654\n",
      "Epoch 34/100\n",
      "16640/16640 [==============================] - 10s 629us/step - loss: 0.0137 - acc: 0.9953 - val_loss: 0.5919 - val_acc: 0.8714\n",
      "Epoch 35/100\n",
      "16640/16640 [==============================] - 11s 645us/step - loss: 0.0145 - acc: 0.9947 - val_loss: 0.6518 - val_acc: 0.8724\n",
      "Epoch 36/100\n",
      "16640/16640 [==============================] - 11s 632us/step - loss: 0.0172 - acc: 0.9936 - val_loss: 0.5990 - val_acc: 0.8683\n",
      "Epoch 37/100\n",
      "16640/16640 [==============================] - 10s 619us/step - loss: 0.0148 - acc: 0.9953 - val_loss: 0.6066 - val_acc: 0.8651\n",
      "Epoch 38/100\n",
      "16640/16640 [==============================] - 10s 617us/step - loss: 0.0150 - acc: 0.9952 - val_loss: 0.5586 - val_acc: 0.8728\n",
      "Epoch 39/100\n",
      "16640/16640 [==============================] - 11s 661us/step - loss: 0.0153 - acc: 0.9942 - val_loss: 0.5532 - val_acc: 0.8764\n",
      "Epoch 40/100\n",
      "16640/16640 [==============================] - 11s 638us/step - loss: 0.0119 - acc: 0.9956 - val_loss: 0.5982 - val_acc: 0.8762\n",
      "Epoch 41/100\n",
      "16640/16640 [==============================] - 11s 633us/step - loss: 0.0127 - acc: 0.9959 - val_loss: 0.6102 - val_acc: 0.8740\n",
      "Epoch 42/100\n",
      "16640/16640 [==============================] - 10s 615us/step - loss: 0.0096 - acc: 0.9960 - val_loss: 0.6425 - val_acc: 0.8716\n",
      "Epoch 43/100\n",
      "16640/16640 [==============================] - 10s 589us/step - loss: 0.0169 - acc: 0.9942 - val_loss: 0.5522 - val_acc: 0.8714\n",
      "Epoch 44/100\n",
      "16640/16640 [==============================] - 10s 623us/step - loss: 0.0128 - acc: 0.9954 - val_loss: 0.5878 - val_acc: 0.8748\n",
      "Epoch 45/100\n",
      "16640/16640 [==============================] - 12s 700us/step - loss: 0.0111 - acc: 0.9960 - val_loss: 0.6165 - val_acc: 0.8750\n",
      "Epoch 46/100\n",
      "16640/16640 [==============================] - 12s 741us/step - loss: 0.0118 - acc: 0.9956 - val_loss: 0.6018 - val_acc: 0.8731\n",
      "Epoch 47/100\n",
      "16640/16640 [==============================] - 10s 603us/step - loss: 0.0097 - acc: 0.9966 - val_loss: 0.6197 - val_acc: 0.8786\n",
      "Epoch 48/100\n",
      "16640/16640 [==============================] - 12s 719us/step - loss: 0.0131 - acc: 0.9954 - val_loss: 0.5846 - val_acc: 0.8781\n",
      "Epoch 49/100\n",
      "16640/16640 [==============================] - 11s 687us/step - loss: 0.0116 - acc: 0.9963 - val_loss: 0.5927 - val_acc: 0.8767\n",
      "Epoch 50/100\n",
      "16640/16640 [==============================] - 11s 654us/step - loss: 0.0100 - acc: 0.9966 - val_loss: 0.6165 - val_acc: 0.8716\n",
      "Epoch 51/100\n",
      "16640/16640 [==============================] - 10s 609us/step - loss: 0.0108 - acc: 0.9966 - val_loss: 0.5822 - val_acc: 0.8707\n",
      "Epoch 52/100\n",
      "16640/16640 [==============================] - 10s 596us/step - loss: 0.0117 - acc: 0.9962 - val_loss: 0.5914 - val_acc: 0.8707\n",
      "Epoch 53/100\n",
      "16640/16640 [==============================] - 10s 617us/step - loss: 0.0094 - acc: 0.9968 - val_loss: 0.6320 - val_acc: 0.8728\n",
      "Epoch 54/100\n",
      "16640/16640 [==============================] - 11s 640us/step - loss: 0.0112 - acc: 0.9963 - val_loss: 0.6673 - val_acc: 0.8740\n",
      "Epoch 55/100\n",
      "16640/16640 [==============================] - 11s 635us/step - loss: 0.0086 - acc: 0.9971 - val_loss: 0.6569 - val_acc: 0.8762\n",
      "Epoch 56/100\n",
      "16640/16640 [==============================] - 10s 583us/step - loss: 0.0121 - acc: 0.9961 - val_loss: 0.5704 - val_acc: 0.8731\n",
      "Epoch 57/100\n",
      "16640/16640 [==============================] - 10s 600us/step - loss: 0.0096 - acc: 0.9966 - val_loss: 0.6961 - val_acc: 0.8673\n",
      "Epoch 58/100\n",
      "16640/16640 [==============================] - 10s 582us/step - loss: 0.0115 - acc: 0.9963 - val_loss: 0.6210 - val_acc: 0.8709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "16640/16640 [==============================] - 10s 575us/step - loss: 0.0107 - acc: 0.9963 - val_loss: 0.6622 - val_acc: 0.8654\n",
      "Epoch 60/100\n",
      "16640/16640 [==============================] - 10s 594us/step - loss: 0.0087 - acc: 0.9971 - val_loss: 0.6159 - val_acc: 0.8738\n",
      "Epoch 61/100\n",
      "16640/16640 [==============================] - 10s 595us/step - loss: 0.0099 - acc: 0.9968 - val_loss: 0.5982 - val_acc: 0.8702\n",
      "Epoch 62/100\n",
      "16640/16640 [==============================] - 10s 622us/step - loss: 0.0100 - acc: 0.9971 - val_loss: 0.6390 - val_acc: 0.8728\n",
      "Epoch 63/100\n",
      "16640/16640 [==============================] - 10s 615us/step - loss: 0.0062 - acc: 0.9977 - val_loss: 0.7485 - val_acc: 0.8668\n",
      "Epoch 64/100\n",
      "16640/16640 [==============================] - 10s 628us/step - loss: 0.0062 - acc: 0.9980 - val_loss: 0.7519 - val_acc: 0.8762\n",
      "Epoch 65/100\n",
      "16640/16640 [==============================] - 10s 627us/step - loss: 0.0090 - acc: 0.9973 - val_loss: 0.6978 - val_acc: 0.8731\n",
      "Epoch 66/100\n",
      "16640/16640 [==============================] - 10s 586us/step - loss: 0.0113 - acc: 0.9962 - val_loss: 0.6473 - val_acc: 0.8748\n",
      "Epoch 67/100\n",
      "16640/16640 [==============================] - 10s 615us/step - loss: 0.0085 - acc: 0.9972 - val_loss: 0.6809 - val_acc: 0.8731\n",
      "Epoch 68/100\n",
      "16640/16640 [==============================] - 11s 652us/step - loss: 0.0071 - acc: 0.9975 - val_loss: 0.6890 - val_acc: 0.8755\n",
      "Epoch 69/100\n",
      "16640/16640 [==============================] - 10s 593us/step - loss: 0.0115 - acc: 0.9961 - val_loss: 0.6240 - val_acc: 0.8714\n",
      "Epoch 70/100\n",
      "16640/16640 [==============================] - 10s 603us/step - loss: 0.0089 - acc: 0.9975 - val_loss: 0.7001 - val_acc: 0.8697\n",
      "Epoch 71/100\n",
      "16640/16640 [==============================] - 10s 580us/step - loss: 0.0083 - acc: 0.9975 - val_loss: 0.6316 - val_acc: 0.8743\n",
      "Epoch 72/100\n",
      "16640/16640 [==============================] - 11s 672us/step - loss: 0.0061 - acc: 0.9978 - val_loss: 0.7419 - val_acc: 0.8709\n",
      "Epoch 73/100\n",
      "16640/16640 [==============================] - 11s 660us/step - loss: 0.0081 - acc: 0.9973 - val_loss: 0.7552 - val_acc: 0.8716\n",
      "Epoch 74/100\n",
      "16640/16640 [==============================] - 11s 648us/step - loss: 0.0091 - acc: 0.9974 - val_loss: 0.7568 - val_acc: 0.8680\n",
      "Epoch 75/100\n",
      "16640/16640 [==============================] - 10s 627us/step - loss: 0.0080 - acc: 0.9971 - val_loss: 0.7033 - val_acc: 0.8728\n",
      "Epoch 76/100\n",
      "16640/16640 [==============================] - 10s 609us/step - loss: 0.0100 - acc: 0.9969 - val_loss: 0.6143 - val_acc: 0.8738\n",
      "Epoch 77/100\n",
      "16640/16640 [==============================] - 10s 584us/step - loss: 0.0084 - acc: 0.9967 - val_loss: 0.6433 - val_acc: 0.8760\n",
      "Epoch 78/100\n",
      "16640/16640 [==============================] - 10s 596us/step - loss: 0.0071 - acc: 0.9974 - val_loss: 0.6470 - val_acc: 0.8736\n",
      "Epoch 79/100\n",
      "16640/16640 [==============================] - 10s 590us/step - loss: 0.0056 - acc: 0.9981 - val_loss: 0.7760 - val_acc: 0.8748\n",
      "Epoch 80/100\n",
      "16640/16640 [==============================] - 10s 590us/step - loss: 0.0076 - acc: 0.9971 - val_loss: 0.7412 - val_acc: 0.8760\n",
      "Epoch 81/100\n",
      "16640/16640 [==============================] - 10s 583us/step - loss: 0.0083 - acc: 0.9970 - val_loss: 0.7505 - val_acc: 0.8798\n",
      "Epoch 82/100\n",
      "16640/16640 [==============================] - 10s 586us/step - loss: 0.0071 - acc: 0.9977 - val_loss: 0.7140 - val_acc: 0.8745\n",
      "Epoch 83/100\n",
      "16640/16640 [==============================] - 10s 597us/step - loss: 0.0109 - acc: 0.9963 - val_loss: 0.6285 - val_acc: 0.8738\n",
      "Epoch 84/100\n",
      "16640/16640 [==============================] - 10s 595us/step - loss: 0.0045 - acc: 0.9983 - val_loss: 0.7330 - val_acc: 0.8748\n",
      "Epoch 85/100\n",
      "16640/16640 [==============================] - 10s 593us/step - loss: 0.0069 - acc: 0.9977 - val_loss: 0.7403 - val_acc: 0.8752\n",
      "Epoch 86/100\n",
      "16640/16640 [==============================] - 10s 580us/step - loss: 0.0071 - acc: 0.9974 - val_loss: 0.7075 - val_acc: 0.8750\n",
      "Epoch 87/100\n",
      "16640/16640 [==============================] - 10s 592us/step - loss: 0.0076 - acc: 0.9975 - val_loss: 0.6698 - val_acc: 0.8728\n",
      "Epoch 88/100\n",
      "16640/16640 [==============================] - 10s 593us/step - loss: 0.0080 - acc: 0.9974 - val_loss: 0.7704 - val_acc: 0.8743\n",
      "Epoch 89/100\n",
      "16640/16640 [==============================] - 12s 694us/step - loss: 0.0089 - acc: 0.9974 - val_loss: 0.6740 - val_acc: 0.8772\n",
      "Epoch 90/100\n",
      "16640/16640 [==============================] - 11s 659us/step - loss: 0.0060 - acc: 0.9981 - val_loss: 0.6834 - val_acc: 0.8745\n",
      "Epoch 91/100\n",
      "16640/16640 [==============================] - 12s 745us/step - loss: 0.0078 - acc: 0.9976 - val_loss: 0.6790 - val_acc: 0.8748\n",
      "Epoch 92/100\n",
      "16640/16640 [==============================] - 12s 708us/step - loss: 0.0077 - acc: 0.9975 - val_loss: 0.7174 - val_acc: 0.8750\n",
      "Epoch 93/100\n",
      "16640/16640 [==============================] - 11s 663us/step - loss: 0.0067 - acc: 0.9980 - val_loss: 0.6790 - val_acc: 0.8760\n",
      "Epoch 94/100\n",
      "16640/16640 [==============================] - 11s 651us/step - loss: 0.0047 - acc: 0.9983 - val_loss: 0.7167 - val_acc: 0.8791\n",
      "Epoch 95/100\n",
      "16640/16640 [==============================] - 10s 624us/step - loss: 0.0054 - acc: 0.9984 - val_loss: 0.6622 - val_acc: 0.8796\n",
      "Epoch 96/100\n",
      "16640/16640 [==============================] - 10s 614us/step - loss: 0.0049 - acc: 0.9984 - val_loss: 0.7848 - val_acc: 0.8779\n",
      "Epoch 97/100\n",
      "16640/16640 [==============================] - 10s 625us/step - loss: 0.0038 - acc: 0.9984 - val_loss: 0.8260 - val_acc: 0.8767\n",
      "Epoch 98/100\n",
      "16640/16640 [==============================] - 11s 637us/step - loss: 0.0091 - acc: 0.9968 - val_loss: 0.7809 - val_acc: 0.8769\n",
      "Epoch 99/100\n",
      "16640/16640 [==============================] - 11s 653us/step - loss: 0.0085 - acc: 0.9975 - val_loss: 0.6265 - val_acc: 0.8786\n",
      "Epoch 100/100\n",
      "16640/16640 [==============================] - 11s 646us/step - loss: 0.0094 - acc: 0.9969 - val_loss: 0.7725 - val_acc: 0.8764\n",
      "Training Accuracy: 1.0000\n",
      "Testing Accuracy:  0.8764\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(train_cnn_data, y_train, epochs=100, batch_size=50,\n",
    "                   validation_data=(test_cnn_data,y_test))\n",
    "loss, accuracy = model.evaluate(train_cnn_data, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(test_cnn_data, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
